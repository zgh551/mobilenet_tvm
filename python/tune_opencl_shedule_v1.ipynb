{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b1ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tvm\n",
    "from tvm import relay, auto_scheduler, rpc\n",
    "# import tvm.relay.testing\n",
    "from tvm.contrib import graph_executor\n",
    "from tvm.auto_scheduler.utils import request_remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7717cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model = onnx.load('../mobilenet/mobilenetv2-7.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae91d41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape_dict:  {'input': (1, 3, 244, 244)}\n"
     ]
    }
   ],
   "source": [
    "input_name = \"input\"\n",
    "input_shape = (1, 3, 244, 244)\n",
    "shape_dict = {input_name: input_shape}\n",
    "print(\"shape_dict: \", shape_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21145b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, params = relay.frontend.from_onnx(mobilenet_model, shape_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce9ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: v9h\n",
      "rpc_host: 192.168.105.70:9190\n",
      "log file: mobilenet-NCHW-B1-opencl-C20000-T21-08-16-09-55.json\n"
     ]
    }
   ],
   "source": [
    "# Also replace this with the device key in your tracker\n",
    "device_key = \"v9h\"\n",
    "rpc_host = \"192.168.105.70\"\n",
    "rpc_port = 9190\n",
    "\n",
    "# Define the neural network and compilation target.\n",
    "network = \"mobilenet\"\n",
    "batch_size = 1\n",
    "layout = \"NCHW\"\n",
    "turn_trials = 20000\n",
    "turn_enable = False\n",
    "preload_log_file = False\n",
    "# Set this to True if you use ndk tools for cross compiling\n",
    "use_ndk = False\n",
    "# Path to cross compiler\n",
    "# os.environ[\"TVM_NDK_CC\"] = \"/usr/bin/aarch64-linux-gnu-g++\"\n",
    "target = tvm.target.Target(\"opencl\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "dtype = \"float32\"\n",
    "log_file = \"%s-%s-B%d-%s-C%s-T%s.json\" % (network, layout, batch_size, target.kind.name, turn_trials, time.strftime('%y-%m-%d-%H-%M',time.localtime(time.time())))\n",
    "print(\"device:\", device_key)\n",
    "print(\"rpc_host: %s:%s\" % (rpc_host, rpc_port))\n",
    "print(\"log file:\", log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de77f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if layout == 'NHWC':\n",
    "    # convert from NCHW to NHWC\n",
    "    desired_layouts = {'nn.conv2d': ['NHWC', 'default']}\n",
    "\n",
    "    # Convert the layout to NHWC\n",
    "    # RemoveUnunsedFunctions is used to clean up the graph.\n",
    "    seq = tvm.transform.Sequential([relay.transform.RemoveUnusedFunctions(),\n",
    "                                    relay.transform.ConvertLayout(desired_layouts)])\n",
    "\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        model = seq(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e09cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remote = request_remote(device_key, rpc_host, rpc_port)\n",
    "# dev = remote.cl()\n",
    "# print(\"device_name:\", dev.device_name)\n",
    "# print(\"compute_version:\", dev.compute_version)\n",
    "# print(\"max_clock_rate:\", dev.max_clock_rate)\n",
    "# print(\"multi_processor_count:\", dev.multi_processor_count)\n",
    "# print(\"max_thread_dimensions:\", dev.max_thread_dimensions)\n",
    "# max_shared_memory_per_block = dev.max_shared_memory_per_block\n",
    "# print(\"max_shared_memory_per_block:\", max_shared_memory_per_block)\n",
    "# max_threads_per_block = dev.max_threads_per_block\n",
    "# print(\"max_threads_per_block:\", max_threads_per_block)\n",
    "# warp_size = dev.warp_size\n",
    "# print(\"warp_size: \", warp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab2a0f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if turn_enable:\n",
    "    max_shared_memory_per_block = 4096\n",
    "    print(\"max_shared_memory_per_block:\", max_shared_memory_per_block)\n",
    "    max_threads_per_block = 512\n",
    "    print(\"max_threads_per_block:\", max_threads_per_block)\n",
    "    warp_size = 2\n",
    "    print(\"warp_size: \", warp_size)\n",
    "\n",
    "    # There is no explicit local memory limition\n",
    "    # so we can use INT32_MAX to disable the check on local_memory.\n",
    "    max_local_memory_per_block = 4096000 # INT32_MAX\n",
    "    print(\"max_local_memory_per_block:\", max_local_memory_per_block)\n",
    "\n",
    "    max_vthread_extent = 2 #int(dev.warp_size / 4) if int(dev.warp_size / 4) > 1 else dev.warp_size\n",
    "    print(\"max_vthread_extent:\", max_vthread_extent)\n",
    "\n",
    "    num_cores = 2\n",
    "    print(\"number of cores:\", num_cores)\n",
    "\n",
    "    vector_unit_bytes = 16\n",
    "    print(\"vector unit bytes:\", vector_unit_bytes)\n",
    "\n",
    "    cache_line_bytes = 64\n",
    "    print(\"cache line bytes:\", cache_line_bytes)\n",
    "    \n",
    "    hardware_params = auto_scheduler.HardwareParams(num_cores, vector_unit_bytes, cache_line_bytes,\n",
    "                                                max_shared_memory_per_block, max_local_memory_per_block,\n",
    "                                                max_threads_per_block, max_vthread_extent, warp_size)\n",
    "    \n",
    "    tasks, task_weights = auto_scheduler.extract_tasks(model[\"main\"], params, target, hardware_params=hardware_params)\n",
    "    \n",
    "    print(\"Begin tuning...\")\n",
    "    if preload_log_file:\n",
    "        load_log_file = \"mobilenet-NCHW-B1-opencl-C3000-T21-08-11-21-39.json\"\n",
    "        print(\"preload file:\", load_log_file)\n",
    "        tuner = auto_scheduler.TaskScheduler(tasks, task_weights, load_log_file=load_log_file)\n",
    "    else:\n",
    "        tuner = auto_scheduler.TaskScheduler(tasks, task_weights)\n",
    "\n",
    "    tune_option = auto_scheduler.TuningOptions(\n",
    "        num_measure_trials=turn_trials,  # change this to 20000 to achieve the best performance\n",
    "        builder=auto_scheduler.LocalBuilder(build_func=\"ndk\" if use_ndk else \"default\"),\n",
    "        runner=auto_scheduler.RPCRunner(\n",
    "            device_key, host=rpc_host, port=rpc_port, repeat=3, timeout=50\n",
    "        ),\n",
    "        measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
    "    )\n",
    "\n",
    "    tuner.tune(tune_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ede6119b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile...\n",
      "Load File: mobilenet-NCHW-B1-opencl-C20000-T21-08-13-20-16.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "fused_nn.conv2d_add\n",
      "Cannot find tuned schedules for target=opencl -keys=opencl,gpu -max_num_threads=256 -thread_warp_size=1, workload_key=[\"9cbaae33fcec002d03bfa28e06021d56\", 1, 192, 16, 16, 64, 192, 1, 1, 1, 64, 1, 1, 1, 64, 16, 16]. A fallback TOPI schedule is used, which may bring great performance regression or even compilation failure. Compute DAG info:\n",
      "placeholder = PLACEHOLDER [1, 192, 16, 16]\n",
      "pad_temp(i0, i1, i2, i3) = placeholder[i0, i1, i2, i3]\n",
      "placeholder = PLACEHOLDER [64, 192, 1, 1]\n",
      "compute(nn, ff, yy, xx) += (pad_temp[nn, rc, (yy + ry), (xx + rx)]*placeholder[ff, rc, ry, rx])\n",
      "placeholder = PLACEHOLDER [1, 64, 1, 1]\n",
      "T_add(ax0, ax1, ax2, ax3) = (compute[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, 0, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the whole network\n",
    "print(\"Compile...\")\n",
    "log_file = \"mobilenet-NCHW-B1-opencl-C20000-T21-08-13-20-16.json\"\n",
    "# log_file = \"mobilenet-NCHW-B1-opencl-C3000-T21-08-11-21-39.json\" # 76ms -> opencl\n",
    "print(\"Load File:\", log_file)\n",
    "with auto_scheduler.ApplyHistoryBest(log_file):\n",
    "    with tvm.transform.PassContext(opt_level=3, config={\"relay.backend.use_auto_scheduler\": True}):\n",
    "        lib = relay.build(model, target, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a5b4613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== Request Remote ===============\n"
     ]
    }
   ],
   "source": [
    "# Create graph executor\n",
    "print(\"=============== Request Remote ===============\")\n",
    "from tvm.auto_scheduler.utils import request_remote\n",
    "remote = request_remote(device_key, rpc_host, rpc_port, timeout = 10000)\n",
    "\n",
    "# tracker = rpc.connect_tracker(rpc_host, rpc_port)\n",
    "# remote = tracker.request(device_key, priority=1, session_timeout=10000)\n",
    "\n",
    "from tvm.contrib import utils, ndk\n",
    "temp = utils.tempdir()\n",
    "filename = \"deploy_lib.tar\"\n",
    "path_lib = temp.relpath(filename)\n",
    "# lib.export_library(path_lib, ndk.create_shared)\n",
    "lib.export_library(path_lib)\n",
    "remote.upload(path_lib)\n",
    "loaded_lib = remote.load_module(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f811024",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = remote.cl()\n",
    "module = graph_executor.GraphModule(loaded_lib[\"default\"](dev))\n",
    "data = (np.random.uniform(size=input_shape)).astype(dtype)\n",
    "data_tvm = tvm.nd.array(data)\n",
    "module.set_input(input_name, data_tvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5202d9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate inference time cost...\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "print(\"Evaluate inference time cost...\")\n",
    "ftimer = module.module.time_evaluator(\"run\", dev, number=50, repeat=3, min_repeat_ms=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7fe5923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean inference time (std dev): 75.83 ms (3.26 ms)\n"
     ]
    }
   ],
   "source": [
    "prof_res = np.array(ftimer().results) * 1e3  # convert to millisecond\n",
    "print(\"Mean inference time (std dev): %.2f ms (%.2f ms)\" % (np.mean(prof_res), np.std(prof_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354d2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd5c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
